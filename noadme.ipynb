{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README? NoADME!\n",
    "\n",
    "## Overview\n",
    "\n",
    "A concept figure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -qU guardrails-ai openai langchain_community langchain_experimental langchain-upstage sentence-transformers langchainhub langchain-chroma langchain matplotlib python-dotenv tavily-python ragas faiss-cpu tokenizers gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "# set UPSTAGE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "from langchain_upstage import ChatUpstage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "\n",
    "\n",
    "llm = ChatUpstage(streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More general chat\n",
    "chat_with_history_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{message}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need prompt code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import PromptTemplate\n",
    "# from langchain_upstage import ChatUpstage\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# prompt_template = PromptTemplate.from_template(\n",
    "#     \"\"\"\n",
    "#     Q: Please extract user and participants's name  from the following text. I am a user. Then, organize the action relationships between me and the participants. \n",
    "\n",
    "#     ---\n",
    "#     We are a team of 5 members called Krispy Team.\n",
    "#     Dr. Kim Yoon-gon gave me coffee. I said thank you.\n",
    "#     Student Jae-min also received coffee from Dr. Kim Yoon-gon. Dr. Son Seok-ho is explaining the team project topic now.\n",
    "#     Student Sung-hoon is preparing presentation materials for other team members.\n",
    "\n",
    "#     A:\n",
    "#     sequenceDiagram\n",
    "#         actor User\n",
    "#         participant Kim Yoon-gon\n",
    "#         participant Jae-min\n",
    "#         participant Son Seok-ho\n",
    "#         participant Sung-hoon\n",
    "#         participant Other team members\n",
    "\n",
    "#         User ->> Kim Yoon-gon : Received coffee and said thank you\n",
    "#         Kim Yoon-gon ->> Jae-min : Gave coffee\n",
    "#         Son Seok-ho ->> Team : Explained the project topic\n",
    "#         Sung-hoon ->> Other team members : Prepared presentation materials\n",
    "\n",
    "#     Q: Please extract user and participants's name  from the following text. I am a user.Then, organize the action relationships between me and the participants. \n",
    "\n",
    "#     ---\n",
    "#     {text}\n",
    "#     \"\"\"\n",
    "# )\n",
    "\n",
    "# llm = ChatUpstage()\n",
    "# chain = prompt_template | llm | StrOutputParser()\n",
    "# content = \"We are a team of four.I am currently creating a template to give to Dr. Kim Yoon-gon.Also, Student Sung-hoon is creating an RAG to give to Dr. Kim Yoon-gon. The other two members are discussing ideas with each other.\"\n",
    "# answer = chain.invoke({\"text\": content})\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need knowledge base and RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_with_history_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    history_langchain_format = []\n",
    "    for human, ai in history:\n",
    "        history_langchain_format.append(HumanMessage(content=human))\n",
    "        history_langchain_format.append(AIMessage(content=ai))\n",
    "\n",
    "    generator = chain.stream({\"message\": message, \"history\": history_langchain_format})\n",
    "\n",
    "    assistant = \"\"\n",
    "    for gen in generator:\n",
    "        assistant += gen\n",
    "        yield assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.ChatInterface(\n",
    "        chat,\n",
    "        examples=[\n",
    "            \"How to eat healthy?\",\n",
    "            \"Best Places in Korea\",\n",
    "            \"How to make a chatbot?\",\n",
    "        ],\n",
    "        title=\"Solar Chatbot\",\n",
    "        description=\"Upstage Solar Chatbot\",\n",
    "    )\n",
    "    chatbot.chatbot.height = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
