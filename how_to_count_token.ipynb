{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c73fbd31-a6c3-4852-bddd-4e722c85ca4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'XXX'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "os.environ.setdefault(\"UPSTAGE_API_KEY\", \"XXX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b6250fd-17aa-4f6b-afbe-ce4e4a8574f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.environ.get(\"UPSTAGE_API_KEY\"), base_url=\"https://api.upstage.ai/v1/solar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "689b43c1-9deb-4b3f-923d-06e12796f2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "11a0eff3-4652-4aab-bced-576e7fa8fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Your name is Solar. You should answer politely.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hi!\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"How can I help you?\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"his tutorial demonstrates how to combine the semantic search results from a $vectorSearch query with full-text search results from a $search query by using reciprocal rank fusion. Reciprocal rank fusion is a way to combine results from different types of searches into a single result. This tutorial takes you through the following steps: Create an Atlas Vector Search index on the plot_embeddings field in the sample_mflix.embedded_movies collection.Create an Atlas Search index on the title field in the sample_mflix.embedded_movies collection. Combine and run a $vectorSearch query against the plot_embeddings field in the sample_mflix.embedded_movies collection with a $search query against the title field by using reciprocal rank fusion.PrerequisitesBefore you begin, ensure that your Atlas cluster meets the requirements described in the Prerequisites. NOTE Ensure that your Atlas cluster has enough memory to store both Atlas Search and Atlas Vector Search indexes and run performant queries. Create the Atlas Vector Search and Atlas Search Indexes This section demonstrates how to create the following indexes on the fields in the sample_mflix.embedded_movies collection: An Atlas Vector Search index on the plot_.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dee230c-b549-4f05-9462-205afcfae526",
   "metadata": {},
   "source": [
    "#### count messages tokens using tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3f5d4d66-cdb3-456e-9551-4ceb330c235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens_from_messages(tokenizer, messages):\n",
    "    tokens_per_message = 5 # <|im_start|>{role}\\n{message}<|im_end|>\n",
    "    tokens_prefix = 1 # <|startoftext|>\n",
    "    tokens_suffix = 3 # <|im_start|>assistant\\n\n",
    "\n",
    "    num_tokens = 0\n",
    "    num_tokens += 1\n",
    "    for m in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for _, v in m.items():\n",
    "            num_tokens += len(tokenizer.encode(v, add_special_tokens=False))\n",
    "    \n",
    "    num_tokens += tokens_suffix\n",
    "    return num_tokens    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0632b2a6-5025-4088-80c2-510f59a9dd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318 prompt tokens counted by the Solar tokenizer\n"
     ]
    }
   ],
   "source": [
    "print(count_tokens_from_messages(tokenizer, messages), 'prompt tokens counted by the Solar tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc98654f-012b-46ac-b59c-11c267c57928",
   "metadata": {},
   "source": [
    "#### count messages tokens using solar llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3cf9d8fd-fbfc-4e71-a4e5-a1cb55df0cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318 prompt tokens counted by the Solar.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example token count from the OpenAI API\n",
    "response = client.chat.completions.create(model=\"solar-1-mini-chat\",\n",
    "                                          messages=messages,\n",
    "                                          temperature=0)\n",
    "print(f'{response.usage.prompt_tokens} prompt tokens counted by the Solar.')\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
